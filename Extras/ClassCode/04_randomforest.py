# -*- coding: utf-8 -*-
"""04_RandomForest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n6a_6NWRaTL_qxQDwD3XkxiBLEqT87gU
"""

# Import pandas and numpy libraries

import pandas as pd
import numpy as np

# Read train data from csv file

train_df = pd.read_csv('Ames_train.csv')
train_df.head()

# For the example codes, I will only use
# 4 variables and the target price
train_df = train_df[['Gr Liv Area',
                     'Garage Area',
                     'Year Built',
                     'Garage Finish',
                     'SalePrice']]

train_df.head()

from sklearn.preprocessing import OneHotEncoder

# We define our One Hot Encoder
ohe = OneHotEncoder(categories='auto', handle_unknown='ignore')

# We can use here a list of one or more categorical variables to convert into numerical
categorical_features = ['Garage Finish']

# We apply the encoding to our train dataset
feature_arr = ohe.fit_transform(train_df[categorical_features]).toarray()

# We also store the names of the categories for the new column names
feature_labels = ohe.categories_

# We apply the new feature names
features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())

# And then combine with the rest of the numerical variables
train_df = pd.concat([train_df, features], axis=1).drop(columns=categorical_features,
                                                        axis=1)

train_df.head()

# Now, I'll keep just my 4 variables (they are 7 now after OHE)
# We will use the standard naming of:
# - X for the dataframe including all the input variables
# - y for the target 1-column dataframe

X = train_df.drop(columns=['SalePrice']).fillna(0)

y = train_df[['SalePrice']]

# We print the shape to get a better idea of the number
# of rows and columns that we got

X.shape, y.shape

# Feature Engineering:

X['Garage_area_ratio'] = X['Garage Area']/X['Gr Liv Area']
X['Total_area'] = X['Garage Area'] + X['Gr Liv Area']

# Engineering more variables by applying non-linear
# transformations to original variables

for column in ['Gr Liv Area',
               'Garage Area',
               'Year Built',
               'Garage_area_ratio',
               'Total_area']:
    X[column + '_log'] = np.log(X[column] + 0.0001)

X.head()

from sklearn.model_selection import train_test_split

# As seen before, we need to create a random split for validation
# in this case, we'll use a 30% split.

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  y,
                                                  test_size=0.3,
                                                  random_state=2023)

X_train.shape, X_val.shape, y_train.shape, y_val.shape

# We'll iterate through different number of estimators,
# features, and max_depth.
# train the model and test it using RMSE
# in both train and validations sets:

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

result_train_dict = {}
result_val_dict = {}

max_max_depth = 14
max_n_estimators = 71

for m in range(1, max_max_depth, 2): # max depth of trees

    print(m) # Just to know how the training is progressing

    for n in range(1, max_n_estimators, 2): # number of trees
        for s in [0.6, 0.7, 0.8, 0.9]:      # % of samples
            for k in [0.6, 0.7, 0.8, 0.9]:  # % of features

                tree_reg = RandomForestRegressor(random_state= 42,
                                                 n_estimators= n,
                                                 max_depth= m,
                                                 max_features= k,
                                                 bootstrap= True,
                                                 max_samples= s)

                tree_reg.fit(X_train, np.ravel(y_train))

                train_predicted = tree_reg.predict(X_train)
                val_predicted = tree_reg.predict(X_val)

                result_train_dict[m, n, s, k] = mean_squared_error(np.log(y_train),
                                                                   np.log(train_predicted),
                                                                   squared = False)
                result_val_dict[m, n, s, k] = mean_squared_error(np.log(y_val),
                                                                 np.log(val_predicted),
                                                                 squared = False)

pd.DataFrame.from_dict(result_val_dict,
                       orient='index',
                       columns=['val_rmse']
                      ).sort_values(by='val_rmse',
                                    ascending=True)

pd.DataFrame.from_dict(result_train_dict, orient='index',
                       columns=['train_rmse']).loc[[(13, 51, 0.6, 0.8)]]

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

fig, ax = plt.subplots(figsize=(15, 7))
colors= ['black', 'red', 'blue', 'green', 'orange', 'gray','navy']
color_index = 0
custom_lines = np.array([])
custom_names = np.array([])

s=0.6
k=0.8


for m in range(1, max_max_depth, 2):

    line_plot_train = []
    line_plot_val = []

    for n in range(1, max_n_estimators, 2):
        line_plot_train.append(result_train_dict.get((m,n,s,k)))
        line_plot_val.append(result_val_dict.get((m,n,s,k)))

    # Un-comment this line for plotting train error evolution
    #plt.plot(np.arange(start=1, stop=max_n_estimators, step=2),
    #         line_plot_train, alpha=0.5, c=colors[color_index], linestyle='--')

    # Un-comment this line for plotting validation error evolution
    plt.plot(np.arange(start=1, stop=max_n_estimators, step=2),
             line_plot_val, alpha=0.5, c=colors[color_index])

    color_line = Line2D([0], [0], color=colors[color_index], lw=4)
    custom_lines = np.append(custom_lines,color_line)
    color_name = 'max_depth =' + str(m)
    custom_names = np.append(custom_names,color_name)
    color_index+=1

#ax.set_xlim(max_min_samples_leaf, 0)
#plt.xticks(np.arange(start=1, stop=max_min_samples_leaf, step=2), rotation=90, size=10)

ax.grid(True)
plt.xlabel('# n_estimators')
plt.ylabel('MSE-Log')

ax.legend(custom_lines, custom_names)

plt.show()

# We have chosen n_estimators=25, max_depth=11, and 60% of both features and samples

tree_reg = RandomForestRegressor(random_state=42,
                                 n_estimators= 51,
                                 max_depth= 13,
                                 max_features= 0.8,
                                 max_samples= 0.6,
                                 bootstrap= True)

tree_reg.fit(X_train, np.ravel(y_train))

train_predicted = tree_reg.predict(X_train)
val_predicted = tree_reg.predict(X_val)

(mean_squared_error(np.log(y_train), np.log(train_predicted), squared = False),
 mean_squared_error(np.log(y_val), np.log(val_predicted), squared = False))

# Apply the model parameters to the whole train dataset
# getting a similar RMSE that we got for training

tree_reg = RandomForestRegressor(random_state=42,
                                 n_estimators= 51,
                                 max_depth= 13,
                                 max_features= 0.8,
                                 max_samples= 0.6,
                                 bootstrap= True)

tree_reg.fit(X, np.ravel(y))

train_predicted = tree_reg.predict(X)

mean_squared_error(np.log(y), np.log(train_predicted), squared = False)

# Read the test csv file, and repeat the process:

test_df = pd.read_csv('Ames_test.csv')
test_df.head()

# Keep the same variables, apply the same transformations

test_df = test_df[['Gr Liv Area',
                   'Garage Area',
                   'Year Built',
                   'Garage Finish',
                   'SalePrice']]

# Careful! For the test set, we just APPLY the One Hote Encoder
# So we only use "transform", not "fit" and "transform".

categorical_features = ['Garage Finish']

feature_arr = ohe.transform(test_df[categorical_features]).toarray()
feature_labels = ohe.categories_

features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())
test_df = pd.concat([test_df, features], axis=1).drop(columns=categorical_features,
                                                      axis=1)

test_df.head()

X_test= test_df.drop(columns=['SalePrice']).fillna(0)

# Feature Engineering:

X_test['Garage_area_ratio'] = X_test['Garage Area']/X_test['Gr Liv Area']
X_test['Total_area'] = X_test['Garage Area'] + X_test['Gr Liv Area']

# Engineering more variables by applying non-linear
# transformations to original variables

for column in ['Gr Liv Area',
               'Garage Area',
               'Year Built',
               'Garage_area_ratio',
               'Total_area']:
    X_test[column + '_log'] = np.log(X_test[column] + 0.0001)

y_test = test_df[['SalePrice']].copy()

# Apply the already previously trained tree to our test dataset:

y_test['SalePrice_predicted'] = tree_reg.predict(X_test)
y_test.head()

# What error are we getting for the test set?

print(mean_squared_error(np.log(y_test['SalePrice']),
                         np.log(y_test['SalePrice_predicted']),
                         squared = False))

feat_importance = tree_reg.feature_importances_
tree_importances = pd.Series(feat_importance, index=X_test.columns).sort_values(ascending=False)

fig, ax = plt.subplots()
tree_importances.plot.bar(ax=ax)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity (normalized)")
fig.tight_layout()

