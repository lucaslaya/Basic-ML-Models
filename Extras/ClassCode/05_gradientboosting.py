# -*- coding: utf-8 -*-
"""05_GradientBoosting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x9DKB3YUqnRR-KO7PpqDdmbVspAiX55t
"""

# Import pandas and numpy libraries

import pandas as pd
import numpy as np

# Read train data from csv file

train_df = pd.read_csv('Ames_train.csv')
train_df.head()

# For the example codes, I will only use
# 4 variables and the target price
train_df = train_df[['Gr Liv Area',
                     'Garage Area',
                     'Year Built',
                     'Garage Finish',
                     'SalePrice']]

train_df.head()

from sklearn.preprocessing import OneHotEncoder

# We define our One Hot Encoder
ohe = OneHotEncoder(categories='auto', handle_unknown='ignore')

# We can use here a list of one or more categorical variables to convert into numerical
categorical_features = ['Garage Finish']

# We apply the encoding to our train dataset
feature_arr = ohe.fit_transform(train_df[categorical_features]).toarray()

# We also store the names of the categories for the new column names
feature_labels = ohe.categories_

# We apply the new feature names
features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())

# And then combine with the rest of the numerical variables
train_df = pd.concat([train_df, features], axis=1).drop(columns=categorical_features,
                                                        axis=1)

train_df.head()

# Now, I'll keep just my 4 variables (they are 7 now after OHE)
# We will use the standard naming of:
# - X for the dataframe including all the input variables
# - y for the target 1-column dataframe

X = train_df.drop(columns=['SalePrice']).fillna(0)

y = train_df[['SalePrice']]

# We print the shape to get a better idea of the number
# of rows and columns that we got

X.shape, y.shape

# Feature Engineering:

X['Garage_area_ratio'] = X['Garage Area']/X['Gr Liv Area']
X['Total_area'] = X['Garage Area'] + X['Gr Liv Area']

# Engineering more variables by applying non-linear
# transformations to original variables

for column in ['Gr Liv Area',
               'Garage Area',
               'Year Built',
               'Garage_area_ratio',
               'Total_area']:
    X[column + '_log'] = np.log(X[column] + 0.0001)

X.head()

from sklearn.model_selection import train_test_split

# As seen before, we need to create a random split for validation
# in this case, we'll use a 30% split.

X_train, X_val, y_train, y_val = train_test_split(X,
                                                  y,
                                                  test_size=0.3,
                                                  random_state=2023)

X_train.shape, X_val.shape, y_train.shape, y_val.shape

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

result_train_dict = {}
result_val_dict = {}
error_curve_train_dict = {}
error_curve_val_dict = {}

for d in [3,5,7]: # max_tree_depth
    for lr in [0.1, 0.05, 0.01]: # learning rate
        for f in [0.3, 0.5, 0.7]: # % of features (columns)
            for s in [0.5, 0.6, 0.7]: # % of data (rows)
                for ms in [10, 20, 30]: # min_samples for split the tree


                    gb_reg = GradientBoostingRegressor(random_state= 28,
                                                       n_estimators= 1000,
                                                       validation_fraction= 0.1,
                                                       n_iter_no_change= 20,
                                                       verbose=0,
                                                       max_depth= d,
                                                       learning_rate= lr,
                                                       max_features= f,
                                                       subsample= s,
                                                       min_samples_split= ms)

                    gb_reg.fit(X_train, np.ravel(y_train))

                    train_predicted = gb_reg.predict(X_train)
                    val_predicted = gb_reg.predict(X_val)

                    result_train_dict[d, lr, f, s, ms] = mean_squared_error(
                        np.log(y_train),
                        np.log(train_predicted),
                        squared = False)
                    result_val_dict[d, lr, f, s, ms] = mean_squared_error(
                        np.log(y_val),
                        np.log(val_predicted),
                        squared = False)

                    error_curve_train_dict[d, lr, f, s, ms] = gb_reg.train_score_

pd.DataFrame.from_dict(result_val_dict, orient='index', columns=['val_rmse']
                      ).sort_values(by='val_rmse', ascending=True)

import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

fig, ax = plt.subplots(figsize=(15, 7))
colors= ['black', 'red', 'blue']
color_index = 0
custom_lines = np.array([])
custom_names = np.array([])

d = 7
f = 0.7
s = 0.5
ms = 30

for lr in [0.1, 0.05, 0.01]:

    plt.plot(error_curve_train_dict[d, lr, f, s, ms],
             alpha=0.5, c=colors[color_index], linestyle='--')

    color_line = Line2D([0], [0], color=colors[color_index], lw=4)
    custom_lines = np.append(custom_lines,color_line)
    color_name = 'learning rate =' + str(lr)
    custom_names = np.append(custom_names,color_name)
    color_index+=1

ax.grid(True)
plt.xlabel('# n_estimators')
plt.ylabel('Error Function')

ax.legend(custom_lines, custom_names)

plt.show()

pd.DataFrame.from_dict(result_train_dict, orient='index',
                       columns=['train_rmse']).loc[[(7, 0.1, 0.7, 0.5, 30)]]

# Apply the model parameters to the whole train dataset
# getting a similar RMSE that we got for training

best = GradientBoostingRegressor(random_state= 28,
                                 n_estimators= 1000,
                                 validation_fraction= 0.1,
                                 n_iter_no_change= 20,
                                 verbose=0,
                                 max_depth= 7,
                                 learning_rate= 0.1,
                                 max_features= 0.7,
                                 subsample= 0.5,
                                 min_samples_split= 30)

best.fit(X.fillna(0), np.ravel(y))

train_predicted = best.predict(X.fillna(0))

mean_squared_error(np.log(y), np.log(train_predicted), squared = False)

plt.plot(best.train_score_)

# Read the test csv file, and repeat the process:

test_df = pd.read_csv('Ames_test.csv')
test_df.head()

# Keep the same variables, apply the same transformations

test_df = test_df[['Gr Liv Area',
                   'Garage Area',
                   'Year Built',
                   'Garage Finish',
                   'SalePrice']]

# Careful! For the test set, we just APPLY the One Hote Encoder
# So we only use "transform", not "fit" and "transform".

categorical_features = ['Garage Finish']

feature_arr = ohe.transform(test_df[categorical_features]).toarray()
feature_labels = ohe.categories_

features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())
test_df = pd.concat([test_df, features], axis=1).drop(columns=categorical_features,
                                                      axis=1)

test_df.head()

X_test= test_df.drop(columns=['SalePrice']).fillna(0)

# Feature Engineering:

X_test['Garage_area_ratio'] = X_test['Garage Area']/X_test['Gr Liv Area']
X_test['Total_area'] = X_test['Garage Area'] + X_test['Gr Liv Area']

# Engineering more variables by applying non-linear
# transformations to original variables

for column in ['Gr Liv Area',
               'Garage Area',
               'Year Built',
               'Garage_area_ratio',
               'Total_area']:
    X_test[column + '_log'] = np.log(X_test[column] + 0.0001)

y_test = test_df[['SalePrice']].copy()

# Apply the already previously trained tree to our test dataset:

y_test['SalePrice_predicted'] = best.predict(X_test.fillna(0))
y_test.head()

# What error are we getting for the test set?

print(mean_squared_error(np.log(y_test['SalePrice']),
                         np.log(y_test['SalePrice_predicted']),
                         squared = False))

feat_importance = best.feature_importances_
tree_importances = pd.Series(feat_importance, index=X_test.columns).sort_values(ascending=False)

import matplotlib.pyplot as plt

fig, ax = plt.subplots()
tree_importances.plot.bar(ax=ax)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity (normalized)")
fig.tight_layout()

