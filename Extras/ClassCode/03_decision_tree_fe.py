# -*- coding: utf-8 -*-
"""03_Decision_Tree_FE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Va_12AlRqIZHynJmhhcxOh2BVMLyA6_
"""

# Import pandas and numpy libraries

import pandas as pd
import numpy as np

# Read train data from csv file

train_df = pd.read_csv('../Group Work/Ames_train.csv') #Cambiar a Ames_train.csv
train_df.head()

# Since we want to use target encoding in my categorical variable, Garage Finish
# let's see first which values we got:

train_df['Garage Finish'].value_counts(dropna=False)

# In the examples, I will use a reduced version of the data, with just 4 variables

X= train_df[['Gr Liv Area',
             'Garage Area',
             'Year Built',
             'Garage Finish']].copy()

# There is one row with no "Garage Area" value, so I'm going to guess it's 0 square feet
# There are 128 rows with no "Garage Finish" value, so I'm creating a "NA" category for them, so we can encode it as any other category

X['Garage Area'] = X['Garage Area'].fillna(0)
X['Garage Finish'] = X['Garage Finish'].fillna('NA')

y = train_df[['SalePrice']]

from sklearn.model_selection import train_test_split

# Create a random 5% split for encoding (the same way we created the validation splits before)
# X_ subsets are for input variables
# y_ subsets are for targets

X, X_encoding, y, y_encoding = train_test_split(X,
                                                y,
                                                test_size=0.05,
                                                random_state=2023)

X.shape, X_encoding.shape, y.shape, y_encoding.shape

# This is how our Encoding table looks like
# We'll substitue the Garage Finish values for their average prices

encoding_lookup = y_encoding.groupby(X_encoding['Garage Finish']).mean()
encoding_lookup

# So, this is how our 4-variable dataset looks like once transformed,
# we'll drop the original categorical variable once we used it for OHE, too.

X_encoded = X.merge(encoding_lookup.reset_index(),
                    how='left',
                    on= 'Garage Finish')

X_encoded.rename(columns={'SalePrice': 'GarageFinishEncoded'}, inplace=True)

X_encoded.head()

# Since Garage Finish is a categorical variable, we define a function for
# applying OHE and drop the original variable
# We'll use the scikit-learn OneHotEncoder:

from sklearn.preprocessing import OneHotEncoder

# We define our One Hot Encoder
ohe = OneHotEncoder(categories='auto', handle_unknown='ignore')

# We can use here a list of one or more categorical variables to convert into numerical
categorical_features = ['Garage Finish']

# We apply the encoding to our train dataset
feature_arr = ohe.fit_transform(X_encoded[categorical_features]).toarray()

# We also store the names of the categories for the new column names
feature_labels = ohe.categories_

# We apply the new feature names
features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())

# And then combine with the rest of the numerical variables
X_encoded = pd.concat([X_encoded, features], axis=1).drop(columns=categorical_features,
                                                        axis=1)

X_encoded.head()

# Creating two new engineered features:

X_encoded['Garage_area_ratio'] = X_encoded['Garage Area']/X_encoded['Gr Liv Area']
X_encoded['Total_area'] = X_encoded['Garage Area'] + X_encoded['Gr Liv Area']

# Creating two new engineered features:

X['Garage_area_ratio'] = X['Garage Area']/X['Gr Liv Area']
X['Total_area'] = X['Garage Area'] + X['Gr Liv Area']

# Engineering more variables by applying non-linear transformations to original variables

for column in ['Gr Liv Area',
               'Garage Area',
               'Year Built',
               'Garage_area_ratio',
               'Total_area']:
    X_encoded[column + '_log'] = np.log(X_encoded[column] + 0.0001)

X_encoded.head()

from sklearn.model_selection import train_test_split

# Create a random 20% split for validation
# X_ subsets are for input variables
# y_ subsets are for targets

X_train, X_val, y_train, y_val = train_test_split(X_encoded.fillna(0),
                                                  y,
                                                  test_size=0.2,
                                                  random_state=2023)

X_encoded.shape, X_train.shape, X_val.shape, y_train.shape, y_val.shape

# We'll iterate through different number of min_samples_leaf and max_depth,
# train the model and test it using RMSE
# in both train and validations sets:

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

result_train_dict = {}
result_val_dict = {}

max_min_samples_leaf = 50
max_max_depth = 15

for m in range(3, max_max_depth+1, 2):
    for n in range(1, max_min_samples_leaf+1, 3):

        tree_reg = DecisionTreeRegressor(random_state=42, min_samples_leaf=n, max_depth=m)
        tree_reg.fit(X_train, y_train)

        train_predicted = tree_reg.predict(X_train)
        val_predicted = tree_reg.predict(X_val)

        result_train_dict[m, n] = mean_squared_error(np.log(y_train), np.log(train_predicted), squared = False)
        result_val_dict[m, n] = mean_squared_error(np.log(y_val), np.log(val_predicted), squared = False)

# We will first choose a good but not overfitting value for max_depth:

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

fig, ax = plt.subplots(figsize=(15, 7))
colors= ['black', 'red', 'blue', 'green', 'orange', 'yellow', 'gray']
color_index = 0
custom_lines = np.array([])
custom_names = np.array([])

x_axis = np.arange(start=1, stop=max_min_samples_leaf+1, step=3)

for m in range(3, max_max_depth+1, 2):

    line_plot_train = []
    line_plot_val = []

    for n in x_axis:
        line_plot_train.append(result_train_dict.get((m,n)))
        line_plot_val.append(result_val_dict.get((m,n)))

    #plt.plot(x_axis, line_plot_train, alpha=0.5, c=colors[color_index], linestyle='--')
    plt.plot(x_axis, line_plot_val, alpha=0.5, c=colors[color_index])

    color_line = Line2D([0], [0], color=colors[color_index], lw=4)
    custom_lines = np.append(custom_lines,color_line)
    color_name = 'max_depth =' + str(m)
    custom_names = np.append(custom_names,color_name)
    color_index+=1

ax.set_xlim(max_min_samples_leaf, 0)
plt.xticks(np.arange(start=1, stop=max_min_samples_leaf, step=2), rotation=90, size=10)

ax.grid(True)
plt.xlabel('# min_samples_leaf')
plt.ylabel('RMSE')

ax.legend(custom_lines, custom_names)

plt.show()

# Once the parameters has been picked, we can fit and predict both train and validation
# and double check that the error values we are getting are the ones in the graph

tree_reg = DecisionTreeRegressor(random_state=42, min_samples_leaf=25, max_depth=11)
tree_reg.fit(X_train, y_train)

train_predicted = tree_reg.predict(X_train)
val_predicted = tree_reg.predict(X_val)

(mean_squared_error(np.log(y_train), np.log(train_predicted), squared = False),
 mean_squared_error(np.log(y_val), np.log(val_predicted), squared = False))

# Apply the model parameters to the whole train dataset
# getting a similar MSE that we got for training

#tree_reg = DecisionTreeRegressor(random_state=42, min_samples_leaf=25, max_depth=11)
#tree_reg.fit(X_encoded.fillna(0), y)

train_predicted = tree_reg.predict(X_encoded.fillna(0))

mean_squared_error(np.log(y), np.log(train_predicted), squared = False)

# Read the test csv file, and repeat the process:

test_df = pd.read_csv('../Group Work/Ames_test.csv')
test_df.head()

# Keep just the variables that we used for training

X_test= test_df[['Gr Liv Area',
                 'Garage Area',
                 'Year Built',
                 'Garage Finish']].copy()

X_test['Garage Area'] = X_test['Garage Area'].fillna(0)
X_test['Garage Finish'] = X_test['Garage Finish'].fillna('NA')

y_test = test_df[['SalePrice']].copy()

# Apply the same transformations

X_test_encoded = X_test.merge(encoding_lookup.reset_index(),
                              how='left',
                              on= 'Garage Finish')

X_test_encoded.rename(columns={'SalePrice': 'GarageFinishEncoded'}, inplace=True)

# Careful! For the test set, we just APPLY the One Hot Encoder
# So we only use "transform", not "fit" and "transform".

categorical_features = ['Garage Finish']

feature_arr = ohe.transform(X_test_encoded[categorical_features]).toarray()
feature_labels = ohe.categories_

features = pd.DataFrame(feature_arr, columns=ohe.get_feature_names_out())
X_test_encoded = pd.concat([X_test_encoded, features], axis=1).drop(columns=categorical_features,
                                                      axis=1)

test_df.head()

X_test_encoded['Garage_area_ratio'] = (X_test_encoded['Garage Area']/
                                       X_test_encoded['Gr Liv Area'])

X_test_encoded['Total_area'] = (X_test_encoded['Garage Area'] +
                                X_test_encoded['Gr Liv Area'])

for column in ['Gr Liv Area', 'Garage Area', 'Year Built', 'Garage_area_ratio', 'Total_area']:
    X_test_encoded[column + '_log'] = np.log(X_test_encoded[column] + 0.0001)

X_test_encoded.head()

# Apply the already previously trained tree to our test dataset:

y_test['SalePrice_predicted'] = tree_reg.predict(X_test_encoded)
y_test.head()

# What error are we getting for the test set?

print(mean_squared_error(np.log(y_test['SalePrice']),
                         np.log(y_test['SalePrice_predicted']),
                         squared = False))

from sklearn import tree
from matplotlib import pyplot as plt
from matplotlib.pyplot import figure

figure(figsize=(20, 20))

tree.plot_tree(tree_reg, max_depth=2, feature_names=X_test_encoded.columns, fontsize=14, label="all", impurity=False)
plt.show()

feat_importance = tree_reg.tree_.compute_feature_importances(normalize=True)
tree_importances = pd.Series(feat_importance, index=X_test_encoded.columns).sort_values(ascending=False)

fig, ax = plt.subplots()
tree_importances.plot.bar(ax=ax)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity (normalized)")
fig.tight_layout()